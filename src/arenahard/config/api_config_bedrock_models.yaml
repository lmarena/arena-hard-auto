# Examples of api config below

#********AWS CLAUDE MODELS *****************#
aws_claude3_haiku:
    model: aws_claude3_haiku
    model_id: anthropic.claude-3-haiku-20240307-v1:0
    endpoints: null
    api_type: aws_claude
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_claude3_sonnet:
    model: aws_claude3_sonnet
    model_id: anthropic.claude-3-sonnet-20240229-v1:0
    endpoints: null
    api_type: aws_claude
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_claude35_sonnet:
    model: aws_claude35_sonnet
    model_id: anthropic.claude-3-5-sonnet-20240620-v1:0
    endpoints: null
    api_type: aws_claude
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_claude3_opus:
    model: aws_claude3_opus
    model_id: anthropic.claude-3-opus-20240229-v1:0
    endpoints: null
    api_type: aws_claude
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

aws_claude35_sonnet_v2:
    model: aws_claude35_sonnet_v2
    model_id: anthropic.claude-3-5-sonnet-20241022-v2:0
    endpoints: null
    api_type: aws_claude
    parallel: 8
    max_tokens: 4096
    temperature: 0.0

aws_claude37_sonnet:
    model: aws_claude37_sonnet
    model_id: us.anthropic.claude-3-7-sonnet-20250219-v1:0 
    endpoints: null
    api_type: aws_claude
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
#********AWS MISTRAL MODELS *****************#    
aws_mistral_7b_instruct:
    model: aws_mistral_7b_instruct
    model_id: mistral.mistral-7b-instruct-v0:2
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_mistral_8x7b_instruct:
    model: aws_mistral_8x7b_instruct
    model_id: mistral.mixtral-8x7b-instruct-v0:1 
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_mistral_large_v1:
    model: aws_mistral_large_v1
    model_id: mistral.mistral-large-2402-v1:0
    endpoints: null
    api_type: aws_mistral
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_mistral_large_v2:
    model: aws_mistral_large_v2
    model_id: mistral.mistral-large-2407-v1:0
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_mistral_small:
    model: aws_mistral_small
    model_id: mistral.mistral-small-2402-v1:0
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0

aws_pixtral_large:
    model: aws_pixtral_large
    model_id: us.mistral.pixtral-large-2502-v1:0
    endpoints: null
    api_type: aws_mistral
    parallel: 8
    max_tokens: 4096
    temperature: 0.0


#********AWS LLAMA MODELS *****************#
aws_llama_3_8b_instruct:
    model: aws_llama_3_8b_instruct
    model_id: meta.llama3-8b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0

aws_llama_3_70b_instruct:
    model: aws_llama_3_70b_instruct
    model_id: meta.llama3-70b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_1_8b_instruct:
    model: aws_llama_3_1_8b_instruct
    model_id: meta.llama3-1-8b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_1_70b_instruct:
    model: aws_llama_3_1_70b_instruct
    model_id: meta.llama3-1-70b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_1_405b_instruct:
    model: aws_llama_3_1_405b_instruct
    model_id: meta.llama3-1-405b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

aws_llama_3_2_1b_instruct:
    model: aws_llama_3_2_1b_instruct
    model_id: meta.llama3-2-1b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
    
aws_llama_3_2_3b_instruct:
    model: aws_llama_3_2_3b_instruct
    model_id: meta.llama3-2-3b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_2_11b_instruct:
    model: aws_llama_3_2_11b_instruct
    model_id: meta.llama3-2-11b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_3_2_90b_instruct:
    model: aws_llama_3_2_90b_instruct
    model_id: meta.llama3-2-90b-instruct-v1:0
    endpoints: null
    api_type: aws_llama
    parallel: 8  
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_2_chat_13b:
    model: aws_llama_2_chat_13b
    model_id: meta.llama2-13b-chat-v1
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_llama_2_chat_70b:
    model: aws_llama_2_chat_70b
    model_id: meta.llama2-70b-chat-v1
    endpoints: null
    api_type: aws_llama
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

#******** AWS NOVA MODELS *****************#
aws_nova_light_v1:
    model: aws_nova_light_v1
    model_id: us.amazon.nova-lite-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_nova_pro_v1:
    model: aws_nova_pro_v1
    model_id: us.amazon.nova-pro-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0
    
aws_nova_micro_v1:
    model: aws_nova_micro_v1
    model_id: us.amazon.nova-micro-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8 
    max_tokens: 4096
    temperature: 0.0

aws_nova_premier_v1: 
    model: aws_nova_premier_v1
    model_id: us.amazon.nova-premier-v1:0
    endpoints: null
    api_type: aws_nova
    parallel: 8
    max_tokens: 4096
    temperature: 0.0
    
#******** AWS DEEPSEEK MODELS *****************#
aws_deepseek-r1_v1:
    model: aws_deepseek-r1_v1
    model_id: us.deepseek.r1-v1:0
    endpoints: null
    api_type: aws_deepseek
    parallel: 8   
    max_tokens: 4096
    temperature: 0.0
